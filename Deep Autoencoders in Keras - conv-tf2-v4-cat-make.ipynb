{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "Model        = tf.keras.models.Model\n",
    "Input        = tf.keras.layers.Input\n",
    "Dense        = tf.keras.layers.Dense\n",
    "Conv2D       = tf.keras.layers.Conv2D\n",
    "MaxPooling2D = tf.keras.layers.MaxPooling2D\n",
    "UpSampling2D = tf.keras.layers.UpSampling2D\n",
    "#mnist        = tf.keras.datasets.mnist\n",
    "save_model   = tf.keras.models.save_model\n",
    "load_model   = tf.keras.models.load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_dim = 64\n",
    "\n",
    "# Architecture\n",
    "layer_input   = Input(shape=(784,))\n",
    "layer_encode1 = Dense(256, activation='relu')(layer_input)\n",
    "layer_encode2 = Dense(128, activation='relu')(layer_encode1)\n",
    "\n",
    "layer_bottleneck = Dense(bottleneck_dim, activation='relu')(layer_encode2)\n",
    "\n",
    "layer_decode1 = Dense(128, activation='relu')(layer_bottleneck)\n",
    "layer_decode2 = Dense(256, activation='relu')(layer_decode1)\n",
    "layer_output  = Dense(784, activation='sigmoid')(layer_decode2)\n",
    "\n",
    "# Autoencoder Model\n",
    "autoencoder = Model(layer_input, layer_output)\n",
    "\n",
    "# Encoder Model\n",
    "encoder = Model(layer_input, layer_bottleneck)\n",
    "\n",
    "# Decoder Architecture\n",
    "Dlayer_input   = Input(shape=(bottleneck_dim,))\n",
    "Dlayer_decode1 = autoencoder.layers[-3](Dlayer_input)\n",
    "Dlayer_decode2 = autoencoder.layers[-2](Dlayer_decode1)\n",
    "Dlayer_output  = autoencoder.layers[-1](Dlayer_decode2) # extract last layer\n",
    "\n",
    "# Decoder Model\n",
    "decoder = Model(Dlayer_input, Dlayer_output)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_square(img):\n",
    "    cols,rows = img.size\n",
    "    \n",
    "    if rows>cols:\n",
    "        pad = (rows-cols)/2\n",
    "        img = img.crop((pad,0,cols,cols))\n",
    "    else:\n",
    "        pad = (cols-rows)/2\n",
    "        img = img.crop((0,pad,rows,rows))\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-224fbc0973ca>, line 37)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-224fbc0973ca>\"\u001b[1;36m, line \u001b[1;32m37\u001b[0m\n\u001b[1;33m    img.load()\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image, ImageFile\n",
    "import requests\n",
    "#import cv2\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "imshow = plt.imshow\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "path = r\"D:\\Public\\fyp-code\\tensorflow2-fyp\\tensorflow2-fyp-compress\\PetImages\\Cat\"\n",
    "\n",
    "#for img in os.listdir(path): # iterate over each image per dogs and cats\n",
    "#    img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_GRAYSCALE) # convert to array # try remove cv2.IMREAD_GRAYSCALE\n",
    "#    plt.imshow(img_array, cmap=\"gray\") # graph it\n",
    "#    plt.show() # display\n",
    "#    break # we just want one for now so break\n",
    "\n",
    "# Checking resized image\n",
    "#IMG_SIZE = 28 # IMPORTANT: must match input size = IMG_SIZE^2\n",
    "\n",
    "#new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "#plt.imshow(new_array, cmap=\"gray\")\n",
    "#plt.show() # TODO fix array resize\n",
    "\n",
    "training_data = []\n",
    "\n",
    "IMAGE_WIDTH = 200\n",
    "IMAGE_HEIGHT = 200\n",
    "IMAGE_CHANNELS = 3\n",
    "\n",
    "for img in os.listdir(path):\n",
    "    ImageFile.LOAD_TRUNCATED_IMAGES = False\n",
    "    print(img)\n",
    "    #response = requests.get(url)\n",
    "    img = Image.open(BytesIO(os.path.join(path,img))\n",
    "    img.load()\n",
    "    img = make_square(img)\n",
    "    img = img.resize((IMAGE_WIDTH,IMAGE_HEIGHT),Image.ANTIALIAS)\n",
    "    training_data.append(np.asarray(img))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping problematic image: 10125.jpg\n",
      "Skipping problematic image: 10501.jpg\n",
      "Skipping problematic image: 10820.jpg\n",
      "Skipping problematic image: 11210.jpg\n",
      "Skipping problematic image: 11565.jpg\n",
      "Skipping problematic image: 11874.jpg\n",
      "Skipping problematic image: 11935.jpg\n",
      "Skipping problematic image: 140.jpg\n",
      "Skipping problematic image: 2663.jpg\n",
      "Skipping problematic image: 3300.jpg\n",
      "Skipping problematic image: 3491.jpg\n",
      "Skipping problematic image: 4833.jpg\n",
      "Skipping problematic image: 5553.jpg\n",
      "Skipping problematic image: 660.jpg\n",
      "Skipping problematic image: 7968.jpg\n",
      "Skipping problematic image: 7978.jpg\n",
      "Skipping problematic image: 8470.jpg\n",
      "Skipping problematic image: 850.jpg\n",
      "Skipping problematic image: 9171.jpg\n",
      "Skipping problematic image: 936.jpg\n",
      "Skipping problematic image: 9565.jpg\n",
      "Skipping problematic image: 9778.jpg\n"
     ]
    }
   ],
   "source": [
    "training_data = []\n",
    "\n",
    "def create_training_data():\n",
    "    #path = os.path.join(DATADIR, category) # path to cats or dogs dir\n",
    "    #class_num = CATEGORIES.index(category)\n",
    "    for img in os.listdir(path):\n",
    "        try:\n",
    "            img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_GRAYSCALE) # try remove cv2.IMREAD_GRAYSCALE\n",
    "            new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "            training_data.append([new_array])\n",
    "            #training_data.append([new_array, class_num]) # todo use this to store original size\n",
    "        except Exception as e:\n",
    "            print('Skipping problematic image: ' + img)\n",
    "            pass\n",
    "\n",
    "create_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12476\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit data\n",
    "autoencoder.fit(training_data, training_data,\n",
    "                epochs=50, batch_size=256, shuffle=True,\n",
    "                validation_data=(training_data, training_data)) # WARINING: RAM will rise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_imgs = encoder.predict(training_data)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(2, n, i+1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    ax = plt.subplot(2, n, i+1+n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28,28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "import math\n",
    "import cv2\n",
    "\n",
    "def psnr(img1, img2):\n",
    "    mse = numpy.mean( (img1 - img2) ** 2 )\n",
    "    if mse == 0:\n",
    "        return 100\n",
    "    PIXEL_MAX = 255.0\n",
    "    return 20 * math.log10(PIXEL_MAX / math.sqrt(mse))\n",
    "\n",
    "n = 10\n",
    "for i in range(n):\n",
    "    original = x_test[i]\n",
    "    contrast = decoded_imgs[i]\n",
    "    d=psnr(original,contrast)\n",
    "    print(\"%.1f\" % d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(autoencoder, \"model/autoencoderv4-784-256-128-64-128-256-784.h5\")\n",
    "save_model(encoder, \"model/encoderv4-784-256-128-64.h5\")\n",
    "save_model(decoder, \"model/decoderv4-64-128-256-784.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
